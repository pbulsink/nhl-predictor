---
title: "Dixon-Coles and Hockey Data"
author: "Philip Bulsink"
date: 
output: html_document
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=12, fig.height=8, fig.path='Figs/',
                      warning=FALSE, message=FALSE, tidy=TRUE)
set.seed(1)
```

```{r echo=FALSE, cache=TRUE, warning=FALSE}
#Import data 
otTagger<-function(row) {
  if (row['OT.SO'] != '') {
    if(row['AG']>row['HG']) {
      return("A")
    }
    else{
      return("H")
    }
  }
  else {
    return("")
  }
}

otFixer<-function(row) {
  H<-as.integer(row['HG'])
  A<-as.integer(row['AG'])
  if(row['OT.Win']=='H') {
    H<-H-1
  }
  else if (row['OT.Win']=='A') {
    A<-A-1
  }
  return(data.frame('HG'=H,'AG'=A))
}

nhlDataPrep<-function(df) {
  df<-df[,c('Date','Visitor','G','Home','G.1','X')]
  colnames(df)<-c("Date", "AwayTeam","AG","HomeTeam","HG","OT.SO")
  df<-df[!is.na(df$AG),]
  df$OT.Win<-apply(df,1,otTagger)
  scores<-do.call("rbind",apply(df,1,otFixer))
  df$AG<-scores$AG
  df$HG<-scores$HG
  df$Date<-as.Date(df$Date)
  return(df)
}

clean_teams<-function(df){
    df[df$Home == "Phoenix Coyotes",]$Home <- "Arizona Coyotes"
    df[df$Visitor == "Phoenix Coyotes",]$Visitor <- "Arizona Coyotes"
    df[df$Home == "Mighty Ducks of Anaheim",]$Home <- "Anaheim Ducks"
    df[df$Visitor == "Mighty Ducks of Anaheim",]$Visitor <- "Anaheim Ducks"
    df[df$Home == "Atlanta Thrashers",]$Home <- "Winnipeg Jets"
    df[df$Visitor == "Atlanta Thrashers",]$Visitor <- "Winnipeg Jets"
}

    
getAndPrepAllData<-function(year_list=c(2006, 2007, 2008, 2009,2010,2011,2012,2013,2014,2015,2016)){
    df<-data.frame(Date=NULL, Visitor=NULL, G=NULL, Home=NULL, G.1=NULL, X=NULL)
    for (year in 1:length(year_list)){
        df<-rbind(df, read.csv(paste('../data/leagues_NHL_',year_list[year],'_games_games.csv', sep=''))[c("Date", "Visitor", "G", "Home", "G.1", "X")])
    }
    try(clean_teams(df), silent=TRUE)
    df<-droplevels(df)
    df<-nhlDataPrep(df)
    return(df)
}

nhl_all<-getAndPrepAllData()
nhl2015<-getAndPrepAllData(year_list = c(2015))
```

Last entry we did some data importing and cleaning of historical NHL data, from 2005 to present. This was in anticipation of performing simulation of games, by the Dixon-Coles method. Much of this entry is not my original work, I've used slightly modified versions of the code available from Jonas at the [opisthokonta.com](http://opisthokonta.net/?p=890) [blog](http://opisthokonta.net/?p=913). I've mixed his [optimized Dixon-Coles method](http://opisthokonta.net/?p=939) and the [time regression method](http://opisthokonta.net/?p=1013) which was a key part of Dixon and Coles' paper. I'll compare the performance of this with and without time regression, the [linear regression approximation](http://opisthokonta.net/?p=927) from the blog, the simple [log5]((https://en.wikipedia.org/wiki/Log5) method, and a 50/50 win/loss method later.

First, Dixon and Coles provide a method to increase the number of low-goal scoring games. A keen eye would notice that the goals scored is a bit biased 'left' compared to the actual Poisson curve:

```{r goals_vs_poisson_plot, echo=FALSE, cache=TRUE}
library(ggplot2)
goals<-data.frame("Goals"=as.integer(nhl_all$AG))
goals$Goals<-cbind(goals$Goals, as.integer(nhl_all$HG))
prand<-data.frame("Goals"=rpois(nrow(goals), lambda=mean(goals$Goals)))
goals$Type<-"Goals per Team"
prand$Type<-"Poisson Distribution"

prand_goals<-rbind(prand,goals)

ggplot(prand_goals, aes(Goals, fill = Type)) + 
    ggtitle("Goals per Team vs Poisson  Distribution") +
    xlab("Goals") +
    scale_x_continuous(breaks=seq(0,10,2)) +
    geom_bar(width=0.8, position = 'dodge')
```

The Dixon-Coles method for adjusting for low scoring games is the tau ($\tau$) function, which we'll use in a bit.
```{r}
tau <- Vectorize(function(xx, yy, lambda, mu, rho){
    if (xx == 0 & yy == 0){return(1 - (lambda*mu*rho))
    } else if (xx == 0 & yy == 1){return(1 + (lambda*rho))
    } else if (xx == 1 & yy == 0){return(1 + (mu*rho))
    } else if (xx == 1 & yy == 1){return(1 - rho)
    } else {return(1)}
})
```

This multiplied by the attack and defence rates of each team being modeled. The numbers become incredibly small, so Jonas adds the log of each instead:

```{r}
DClogLik <- function(y1, y2, lambda, mu, rho=0){
    #rho=0, independence
    #y1: home goals
    #y2: away goals
    sum(log(tau(y1, y2, lambda, mu, rho)) + log(dpois(y1, lambda)) + log(dpois(y2, mu)))
}
```

These two functions are used below, when setting up the model to be optimized, and in the model itself. The reader is encouraged to visit the opisthokonta blog to learn more about what happens in these steps:

```{r}
DCmodelData <- function(df){
    
    team.names <- unique(c(levels(df$HomeTeam), levels(df$AwayTeam)))
    
    # attack, with sum-to-zero constraint
    ## home
    hm.a <- model.matrix(~ HomeTeam - 1, data=df)
    hm.a[df$HomeTeam == team.names[length(team.names)], ] <- -1
    hm.a <- hm.a[,1:(length(team.names)-1)]
    
    # away
    am.a <- model.matrix(~ AwayTeam -1, data=df)
    am.a[df$AwayTeam == team.names[length(team.names)], ] <- -1
    am.a <- am.a[,1:(length(team.names)-1)]
    
    # defence, same as before 
    hm.d <- model.matrix(~ HomeTeam - 1, data=df)
    am.d <- model.matrix(~ AwayTeam -1, data=df)
    
    return(list(homeTeamDMa=hm.a, homeTeamDMd=hm.d,
                awayTeamDMa=am.a, awayTeamDMd=am.d,
                homeGoals=df$HG, awayGoals=df$AG,
                teams=team.names)) 
}

DCoptimFn <- function(params, DCm){
    
    home.p <- params[1]
    rho.p <- params[2]
    
    nteams <- length(DCm$teams)
    attack.p <- matrix(params[3:(nteams+1)], ncol=1) #one column less
    defence.p <- matrix(params[(nteams+2):length(params)], ncol=1) 
    
    # need to multiply with the correct matrices
    lambda <- exp(DCm$homeTeamDMa %*% attack.p + DCm$awayTeamDMd %*% defence.p + home.p)
    mu <- exp(DCm$awayTeamDMa %*% attack.p + DCm$homeTeamDMd %*% defence.p)
    
    return(
        DClogLik(y1=DCm$homeGoals, y2=DCm$awayGoals, lambda, mu, rho.p) * -1
    )
}
```

I've taken the lines of code scattered through the opisthokonta blog posts and put them all together in two more easy-use functions:
```{r}
doDCPrediction<-function(df){
    #Get a useful data set
    dcm<-DCmodelData(df)
    nteams <- length(dcm$teams)
    
    #dummy fill parameters
    #initial parameter estimates
    attack.params <- rep(.1, times=nteams-1) # one less parameter
    defence.params <- rep(-0.8, times=nteams)
    home.param <- 0.06
    rho.init <- 0.03
    par.inits <- c(home.param, rho.init, attack.params, defence.params)
    
    #informative names
    #skip the last team
    names(par.inits) <- c('HOME', 'RHO', 
                          paste('Attack', dcm$teams[1:(nteams-1)], sep='.'),
                          paste('Defence', dcm$teams, sep='.'))
    
    res <- optim(par=par.inits, fn=DCoptimFn, DCm=dcm, method='BFGS')
    
    parameters <- res$par
    
    #compute last team attack parameter
    missing.attack <- sum(parameters[3:(nteams+1)]) * -1
    
    #put it in the parameters vector
    parameters <- c(parameters[1:(nteams+1)], missing.attack, parameters[(nteams+2):length(parameters)])
    names(parameters)[nteams+2] <- paste('Attack.', dcm$teams[nteams], sep='')
    
    #increase attack by one
    parameters[3:(nteams+2)] <- parameters[3:(nteams+2)] + 1  
    
    #decrease defence by one
    parameters[(nteams+3):length(parameters)] <- parameters[(nteams+3):length(parameters)] - 1 
    
    res$par<-parameters
    return(res)
    
}
```

Use of this is to feed in the data into the `doDCPrediction` function, returning the `res` parameter object. This is a long calculation. We'll use this result to simulate an actual game in the next post.


```{r all_res, warning=FALSE, cache=TRUE, echo=FALSE}
t<-proc.time()
nhl.all.res<-doDCPrediction(nhl_all)
t_all_calc<-proc.time()-t
```

```{r one_res, warning=FALSE, cache=TRUE, echo=FALSE}
t<-proc.time()
nhl.one.res<-doDCPrediction(nhl2015)
t_one_calc<-proc.time()-t
```

The full 10 seasons took `r as.integer(t_all_calc[3])` seconds, while even just one season took `r as.integer(t_one_calc[3])` seconds. We haven't even added in the promised time dependancy yet! WOW. Hopefully it's a good model. 

The effect that time dependance has on the Poisson determination is weighting more recent games higher than older results. There are a few reasons why this matters, including the effect of short term hot and cold streaks, but also to account for how teams change over time (player quality changes due to age, players added and removed by trades, drafts, or retirement; changes in coaches or coaching style, etc). To add this to our method from above, we need to start with adding a weighting function based on $\xi$ (xi):

```{r}
DCweights <- function(dates, currentDate=Sys.Date(), xi=0){
    datediffs <- dates - as.Date(currentDate)
    datediffs <- as.numeric(datediffs *-1)
    w <- exp(-1*xi*datediffs)
    w[datediffs <= 0] <- 0 #Future dates should have zero weights
    return(w)
}
```

We can see the effect this has by throwing the list of dates from our scores into the function, with different values of $\xi$.

```{r xi_plot, cache=TRUE, echo=FALSE}
#Turn this into a shiny app
library(reshape2)
xi_list<-c(0.0001, 0.001, 0.002, 0.003, 0.005, 0.01, 0.03, 0.05, 0.1, 0.3)
weights <- data.frame(lapply(xi_list, function(x){DCweights(nhl_all$Date, xi=x)}))
colnames(weights)<-xi_list
weights$Date<-nhl_all$Date
m_weights<-melt(weights, id.vars="Date")
ggplot(m_weights, aes(Date, value, color=variable))+geom_point(size=0.5)
```

This graph shows the weight (0 to 1) of a game on a certain date. The lower the weight, the less it impacts the model's optimization. The graph is choppy, because weights are given for games, there's no games played in summer, and in 2012 there was a lockout (note the exta large gap). 

While it might make sense to use a $\xi$ value of 0.005 (focusing mostly on this season, somewhat on the last, but not much on seasons before that), evaulation of the performance of the model at each $\xi$ value would be the best determiner of what to use. Again, we'll look at that later.

With the weighting function available, we'll modify the rest of the functions to use the weights:

```{r}
DClogLik <- function(y1, y2, lambda, mu, rho=0, weights=NULL){
    #rho=0, independence
    #y1 home goals
    #y2 away goals
    loglik <- log(tau(y1, y2, lambda, mu, rho)) + log(dpois(y1, lambda)) + log(dpois(y2, mu))
    if (is.null(weights)){
        return(sum(loglik))
    } else {
        return(sum(loglik*weights))
    }
}

DCmodelData <- function(df){
    team.names <- unique(c(levels(df$HomeTeam), levels(df$AwayTeam)))
    
    # attack, with sum-to-zero constraint
    ## home
    hm.a <- model.matrix(~ HomeTeam - 1, data=df)
    hm.a[df$HomeTeam == team.names[length(team.names)], ] <- -1
    hm.a <- hm.a[,1:(length(team.names)-1)]
    
    # away
    am.a <- model.matrix(~ AwayTeam -1, data=df)
    am.a[df$AwayTeam == team.names[length(team.names)], ] <- -1
    am.a <- am.a[,1:(length(team.names)-1)]
    
    # defence, same as before 
    hm.d <- model.matrix(~ HomeTeam - 1, data=df)
    am.d <- model.matrix(~ AwayTeam -1, data=df)
    
    return(list(homeTeamDMa=hm.a, homeTeamDMd=hm.d,
                awayTeamDMa=am.a, awayTeamDMd=am.d,
                homeGoals=df$HG, awayGoals=df$AG,
                dates=df$Date,
                teams=team.names)) 
}

DCoptimFn <- function(params, DCm, xi=0){
    home.p <- params[1]
    rho.p <- params[2]
    
    nteams <- length(DCm$teams)
    attack.p <- matrix(params[3:(nteams+1)], ncol=1) #one column less
    defence.p <- matrix(params[(nteams+2):length(params)], ncol=1) 
    
    # need to multiply with the correct matrices
    lambda <- exp(DCm$homeTeamDMa %*% attack.p + DCm$awayTeamDMd %*% defence.p + home.p)
    mu <- exp(DCm$awayTeamDMa %*% attack.p + DCm$homeTeamDMd %*% defence.p)
    
    w<-DCweights(DCm$dates, xi=xi)
    return(
        DClogLik(y1=DCm$homeGoals, y2=DCm$awayGoals, lambda, mu, rho.p, w) * -1
    )
}

doDCPrediction<-function(df, xi=0){
    #Get a useful data set
    dcm<-DCmodelData(df)
    nteams <- length(dcm$teams)
    
    #dummy fill parameters
    #initial parameter estimates
    attack.params <- rep(.1, times=nteams-1) # one less parameter
    defence.params <- rep(-0.8, times=nteams)
    home.param <- 0.06
    rho.init <- 0.03
    par.inits <- c(home.param, rho.init, attack.params, defence.params)
    
    #informative names
    #skip the last team
    names(par.inits) <- c('HOME', 'RHO', 
                          paste('Attack', dcm$teams[1:(nteams-1)], sep='.'),
                          paste('Defence', dcm$teams, sep='.'))
    
    res <- optim(par=par.inits, fn=DCoptimFn, DCm=dcm, xi=xi, method='BFGS')
    
    parameters <- res$par
    
    #compute last team attack parameter
    missing.attack <- sum(parameters[3:(nteams+1)]) * -1
    
    #put it in the parameters vector
    parameters <- c(parameters[1:(nteams+1)], missing.attack, parameters[(nteams+2):length(parameters)])
    names(parameters)[nteams+2] <- paste('Attack.', dcm$teams[nteams], sep='')
    
    #increase attack by one
    parameters[3:(nteams+2)] <- parameters[3:(nteams+2)] + 1  
    
    #decrease defence by one
    parameters[(nteams+3):length(parameters)] <- parameters[(nteams+3):length(parameters)] - 1 
    
    res$par<-parameters
    return(res)
}
```

Remaking those functions took a lot of space, but only a few lines changed. We multiply the log likelyhood function by weights, and the remainder is to bring the calculated weights from the user through the model. 

```{r xi_all_res, warning=FALSE, cache=TRUE, echo=FALSE}
t<-proc.time()
nhl_all_res<-doDCPrediction(nhl_all, xi=0.005)
t_all_calc<-proc.time()-t
```

```{r xi_one_res, warning=FALSE, cache=TRUE, echo=FALSE}
t<-proc.time()
nhl_one_res<-doDCPrediction(nhl2015, xi=0.005)
t_one_calc<-proc.time()-t
```

Running it again, we get 10 seasons taking `r as.integer(t_all_calc[3])` seconds, and one season `r as.integer(t_one_calc[3])` seconds. Just a little longer, as we expected.. We could optimize that futher if we wanted (eg. drop data for weight of less than some amount), but fortunately we only have to run this once to start predicting all sorts of results.

We'll plot the Attack and Defence parameters for each team to se if there's any correllation.
```{r attack_defence_corr_plot, cache=TRUE, echo=FALSE}
team_params<-data.frame("Team"=unique(nhl2015$AwayTeam), "Attack"=0, "Defence"=0)
for (t in 1:length(as.character(unique(nhl2015$AwayTeam)))){
     team<-as.character(unique(nhl2015$AwayTeam))[t]
     team_params[team_params$Team == team,]$Attack=nhl_one_res$par[paste("Attack", team, sep=".")]
     team_params[team_params$Team == team,]$Defend=nhl_one_res$par[paste("Defence", team, sep=".")]
}

ggplot(team_params, aes(x=Attack, y=Defence, color=Team)) +
    ggtitle("Attack vs Defence Parameters") +
    xlab("Defence") + 
    ylab("Attack") +
    geom_point() +
    geom_smooth(method="lm")
```

Next time we'll run them and predict some games!


